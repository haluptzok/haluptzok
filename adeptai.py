# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vdyRkrOrwlvh04P65yMA9YYdfjcG3-vo
"""

# Interview Question:
# Purposely very vague - given some x,y data fit a function to it.
# The question was asked to do it in the simple Python approach (not numpy or torch)
# to test for ML math for neural networks basics by seeing if you can derive the
# define the loss function and derive the gradients on the whiteboard and then
# code it up.

# Solution:
# Write a function train(...) that fits f so y = f(x) minimizing MSE loss.
# Data is a list of [[x,y]..] samples
# f(x) is a simple model x -> hidden_layer_sigmoid -> y
# Model is chosen to be something simple that can be finished in interview,
# not the most optimal architecture possible for x,y regression.
#
# Reduce loss doing simple gradient descent, 1 sample at a time (batch_size = 1)
# For weights: Each sigmoid has a,b for exp(-(a+b*x)) and c for combining
# sigmoids into output y. So number of parameters is num_hidden_units * 3
#
# 4 ways to do it:
# train_python: Do it with python (no numpy, no torch) (batch_size=1)
# train_numpy:  Do it with numpy (no torch) (batch_size=1)
# train_torch:  Do it with torch (no Dataset or Dataloader) (batch_size=1)
# train_torch_batch: Do it with pytorch (batch_size=settable)
#
# By setting the random seeds, and doing the exact same random weight init,
# all 4 models converge to exact same weights, and the exact same final loss.
# Kind of fun to see python == numpy == pytorch convergence.
# train_torch_batch also the exact same results if batch_size=1
#
# 2 more ways to do it - mse and weights match exactly for these 3:
# train_torch_vectorized: Do it batch_size=len(data), all data at once
# train_numpy_vectorized: Do it batch_size=len(data), all data at once
# train_torch_batch: Also the exact same results if batch_size=len(data)
#
# A continuum of simple verbose Python to complex succint Pytorch
#

# train_python: Do it in pure simple python
import time
import random
import math

bTest = True
if bTest:
  # make it repro results while debugging
  random.seed(42)

# simple dataset to fit
data = [[0,0], [1,1], [2,2], [3,3], [4,4], [5,5], [6,6], [7,7], [8,8], [9,9]]

def sigmoid(x):
  return 1/(1 + math.exp(-x))

def train_python(data, num_hidden_units = 10, num_epochs=10000, learn_rate=0.001):
  time_start = time.time()
  # random initialize the weights
  a = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
  b = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
  c = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
  sig_unit = [0.0 for _ in range(num_hidden_units)] # just allocate space once for perf

  for epoch in range(num_epochs):
    epoch_loss = 0.0
    for pair in data:
      # forward
      output = 0.0
      for unit in range(num_hidden_units):
        sig_value = sigmoid(a[unit] + (b[unit] * pair[0]))
        sig_unit[unit] = sig_value
        output += c[unit] * sig_value
      # compute loss function
      err = pair[1] - output
      epoch_loss += (err * err)
      # backward - compute gradient
      for unit in range(num_hidden_units):
        # backward - deriv of sigmoid(x) = sigmoid(x) * (1 - sigmoid(x))
        dE_da = -2.0 * err * c[unit] * sig_unit[unit] * (1 - sig_unit[unit])
        dE_db = -2.0 * err * c[unit] * pair[0] * sig_unit[unit] * (1 - sig_unit[unit])
        dE_dc = -2.0 * err * sig_unit[unit]
        # optimizer step - SGD
        a[unit] -= learn_rate * dE_da
        b[unit] -= learn_rate * dE_db
        c[unit] -= learn_rate * dE_dc

    if epoch == 0 or epoch % 1000 == 999:
      print(f"{epoch=} {epoch_loss=}")

  print(f"{a=}\n{b=}\n{c=}")
  time_end = time.time()
  time_diff = time_end - time_start
  print(f"Took {time_diff:.3f} seconds {(time_diff/60):.3f} minutes {(time_diff/3600):.3f} hours.")

train_python(data)

# train_numpy: Do it in numpy
# Note we can init the weights like python - just to show we then get the exact same results as in python
# As in we converge to the exact same weights, exact same MSE as full python example
import time
import random
import math
import numpy as np

bTest = True
if bTest:
  # make it repro results while debugging
  random.seed(42)
  np.random.seed(42)

# simple dataset to fit
data = np.array([[0,0], [1,1], [2,2], [3,3], [4,4], [5,5], [6,6], [7,7], [8,8], [9,9]])

def sigmoid(x):
  return 1/(1 + np.exp(-x))

def train_numpy(data, num_hidden_units = 10, num_epochs=10000, learn_rate=0.001):
  time_start = time.time()
  # random initialize the weights
  if bTest:
    # random weight init like above so it's exact match
    a = np.array([random.gauss(0.0, 1.0) for _ in range(num_hidden_units)])
    b = np.array([random.gauss(0.0, 1.0) for _ in range(num_hidden_units)])
    c = np.array([random.gauss(0.0, 1.0) for _ in range(num_hidden_units)])
  else:
    a = np.random.randn(num_hidden_units)
    b = np.random.randn(num_hidden_units)
    c = np.random.randn(num_hidden_units)

  for epoch in range(num_epochs):
    epoch_loss = 0.0
    for pair in data:
      x, y = pair[0], pair[1]
      # forward
      # x is scalar, a and b are arrays of len num_hidden_units
      sig_vec = sigmoid(a + b * x)
      output_vec = (c * sig_vec).sum()
      # compute scalar loss
      err = y - output_vec
      epoch_loss += (err * err)
      # backward - compute gradient vectors
      dE_da = -2.0 * err * c * sig_vec * (1.0 - sig_vec)
      dE_db = -2.0 * err * c * x * sig_vec * (1.0 - sig_vec)
      dE_dc = -2.0 * err * sig_vec
      # optimizer step - SGD - update weight vectors
      a -= learn_rate * dE_da
      b -= learn_rate * dE_db
      c -= learn_rate * dE_dc
    if epoch == 0 or epoch % 1000 == 999:
      print(f"{epoch=} {epoch_loss=}")

  print(f"{a=}\n{b=}\n{c=}")
  time_end = time.time()
  time_diff = time_end - time_start
  print(f"Took {time_diff:.3f} seconds {(time_diff/60):.3f} minutes {(time_diff/3600):.3f} hours.")

train_numpy(data)

# train_torch: Do it in pytorch
# Note we init the weights like python/numpy - just to show we then get the exact same results as in python/numpy
# As in we converge to the exact same weights, exact same MSE as full python/numpy example
import time
import random
import math
import numpy as np
import torch
from torch.nn.modules import linear
print("torch.__version__", torch.__version__)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"{device=}")
torch.set_default_device(device)

bTest = True
if bTest:
  # make it repro results while debugging
  random.seed(42)
  np.random.seed(42)
  torch.manual_seed(42)

# simple dataset to fit
data = torch.tensor([[0,0], [1,1], [2,2], [3,3], [4,4], [5,5], [6,6], [7,7], [8,8], [9,9]], dtype=torch.float32)

def train_torch(data, num_hidden_units = 10, num_epochs=10000, learn_rate=0.001):
  time_start = time.time()
  model = torch.nn.Sequential(
    torch.nn.Linear(1, num_hidden_units),  # a + bx, a is the bias, b is the weight
    torch.nn.Sigmoid(),
    torch.nn.Linear(num_hidden_units, 1, bias=False)   # c * sigmoid(a + bx)
    # to be 100% equivalent to python&numpy networks above we can't have a bias in last Linear layer
  )

  if bTest:
    # copy weight init from numpy example so it's exact match for debugging:
    a = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    b = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    c = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    with torch.no_grad():
      for i in range(num_hidden_units):
        model[0].bias[i] = a[i]
        model[0].weight[i, 0] = b[i]
        model[2].weight[0, i] = c[i]

  loss_fn = torch.nn.MSELoss(reduction='sum')
  optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)

  for epoch in range(num_epochs):
    epoch_loss = 0.0
    for pair in data:
      x, y = pair[0], pair[1]
      # forward
      xx = torch.reshape(x, (1,1))
      yy = torch.reshape(y, (1,1))
      output_vec = model(xx)
      loss = loss_fn(output_vec, yy)
      with torch.no_grad():
        epoch_loss += loss.item()
      # backward - compute gradient vectors
      optimizer.zero_grad()
      loss.backward()
      # optimizer step - SGD - update weight vectors
      optimizer.step()
    if epoch == 0 or epoch % 1000 == 999:
      print(f"{epoch=} {epoch_loss=}")

  linear_layer = model[0]
  print(f'a= {linear_layer.bias}')
  print(f'b= {linear_layer.weight}')
  linear_layer = model[2]
  print(f'c= {linear_layer.weight}')
  time_end = time.time()
  time_diff = time_end - time_start
  print(f"Took {time_diff:.3f} seconds {(time_diff/60):.3f} minutes {(time_diff/3600):.3f} hours.")

train_torch(data)

# train_torch_vectorized: Do it in pytorch vectorized, batch_size=len(data), all data at once
# Note we init the weights like python/numpy - but this is gradient over whole dataset - not 1 sample at a time per update
# So not the exact same convergence - even starting with same weights.

import time
import random
import math
import numpy as np
import torch
from torch.nn.modules import linear
print("torch.__version__", torch.__version__)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"{device=}")
torch.set_default_device(device)

bTest = True
if bTest:
  # make it repro results while debugging
  random.seed(42)
  np.random.seed(42)
  torch.manual_seed(42)

# simple dataset to fit
data = torch.tensor([[0,0], [1,1], [2,2], [3,3], [4,4], [5,5], [6,6], [7,7], [8,8], [9,9], [10,10], [11,11]], dtype=torch.float32)

def train_torch_vectorized(data, num_hidden_units = 10, num_epochs=10000, learn_rate=0.001):
  time_start = time.time()
  model = torch.nn.Sequential(
    torch.nn.Linear(1, num_hidden_units),  # a + bx, a is the bias, b is the weight
    torch.nn.Sigmoid(),
    torch.nn.Linear(num_hidden_units, 1, bias=False)   # c * sigmoid(a + bx)
    # to be 100% equivalent to python&numpy&pytorch-halfway networks above we can't have a bias in last Linear layer
  )

  if bTest:
    # copy weight init from numpy example so it's exact match for debugging:
    a = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    b = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    c = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    with torch.no_grad():
      for i in range(num_hidden_units):
        model[0].bias[i] = a[i]
        model[0].weight[i, 0] = b[i]
        model[2].weight[0, i] = c[i]

  loss_fn = torch.nn.MSELoss(reduction='sum')
  optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)
  # Do all the data at one time - same for each epoch
  x, y = data[:,0], data[:, 1]
  x = torch.reshape(x, (len(data),1))
  y = torch.reshape(y, (len(data),1))

  for epoch in range(num_epochs):
    output_vec = model(x)
    loss = loss_fn(output_vec, y)
    # backward - compute gradient vectors
    optimizer.zero_grad()
    loss.backward()
    # optimizer step - SGD - update weight vectors
    optimizer.step()
    if epoch == 0 or epoch % 1000 == 999:
      print(f"{epoch=} loss={loss.item()}")

  linear_layer = model[0]
  print(f'a= {linear_layer.bias}')
  print(f'b= {linear_layer.weight}')
  linear_layer = model[2]
  print(f'c= {linear_layer.weight}')
  time_end = time.time()
  time_diff = time_end - time_start
  print(f"Took {time_diff:.3f} seconds {(time_diff/60):.3f} minutes {(time_diff/3600):.3f} hours.")

train_torch_vectorized(data)

# train_numpy_vectorized: Do it in numpy vectorized, batch_size=len(data), all data at once
# Note we init the weights like python/numpy - but this is gradient over whole dataset - not 1 sample at a time per update
# So not the exact same convergence - even starting with same weights.
# But it is the same convergence as torch_vectorized - had to make data and weights float32 or else they diverged a bit.
import time
import random
import math
import numpy as np

bTest = True
if bTest:
  # make it repro results while debugging
  random.seed(42)
  np.random.seed(42)

# simple dataset to fit
data = np.array([[0,0], [1,1], [2,2], [3,3], [4,4], [5,5], [6,6], [7,7], [8,8], [9,9], [10,10], [11,11]], dtype=np.float32)

def sigmoid(x):
  return 1/(1 + np.exp(-x))

def train_numpy_vectorized(data, num_hidden_units = 10, num_epochs=10000, learn_rate=0.001):
  time_start = time.time()
  # random initialize the weights
  if bTest:
    # random weight init like above so it's exact match
    a = np.array([[random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]], dtype=np.float32)
    b = np.array([[random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]], dtype=np.float32)
    c = np.array([[random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]], dtype=np.float32)
  else:
    a = np.random.randn(1, num_hidden_units)
    b = np.random.randn(1, num_hidden_units)
    c = np.random.randn(1, num_hidden_units)

  x, y = data[:,[0]], data[:,[1]]

  for epoch in range(num_epochs):
    # forward
    sig_vec = sigmoid(a + b * x) # shape=len(data) X num_hidden_units
    output_vec = (c * sig_vec).sum(axis=1, keepdims=True)
    # compute scalar loss
    err = y - output_vec
    epoch_loss = (err * err).sum()
    # backward - compute gradient vectors
    dE_da = -2.0 * err * c * sig_vec * (1.0 - sig_vec)
    dE_db = -2.0 * err * c * x * sig_vec * (1.0 - sig_vec)
    dE_dc = -2.0 * err * sig_vec
    # optimizer step - SGD - update weight vectors
    a -= learn_rate * dE_da.sum(axis=0,keepdims=True)
    b -= learn_rate * dE_db.sum(axis=0,keepdims=True)
    c -= learn_rate * dE_dc.sum(axis=0,keepdims=True)
    if epoch == 0 or epoch % 1000 == 999:
      print(f"{epoch=} {epoch_loss=}")

  print(f"{a=}\n{b=}\n{c=}")
  time_end = time.time()
  time_diff = time_end - time_start
  print(f"Took {time_diff:.3f} seconds {(time_diff/60):.3f} minutes {(time_diff/3600):.3f} hours.")

train_numpy_vectorized(data)

# train_torch_batch: Do it in pytorch with batch_size param
# Note we init the weights like python/numpy - but this is gradient over a batch - not 1 sample at a time per update
# So not the exact same convergence - even starting with same weights.

import time
import random
import math
import numpy as np
import torch
from torch.nn.modules import linear
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
print("torch.__version__", torch.__version__)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"{device=}")
torch.set_default_device(device)

bTest = True
if bTest:
  # make it repro results while debugging
  random.seed(42)
  np.random.seed(42)
  torch.manual_seed(42)

# simple dataset to fit
data = torch.tensor([[0,0], [1,1], [2,2], [3,3], [4,4], [5,5], [6,6], [7,7], [8,8], [9,9]], dtype=torch.float32)
# data = torch.tensor([[0,0], [1,1], [2,2], [3,3], [4,4], [5,5], [6,6], [7,7], [8,8], [9,9], [10,10], [11,11]], dtype=torch.float32)

class SimpleDataset(Dataset):
    def __init__(self, data_xy):
        self.data_xy = data_xy

    def __len__(self):
        return self.data_xy.shape[0]

    def __getitem__(self, idx):
        x = self.data_xy[idx][0]
        y = self.data_xy[idx][1]
        return x, y

def train_torch_batch(data, num_hidden_units = 10, num_epochs=10000, learn_rate=0.001, batch_size=10):
  time_start = time.time()
  model = torch.nn.Sequential(
    torch.nn.Linear(1, num_hidden_units),  # a + bx, a is the bias, b is the weight
    torch.nn.Sigmoid(),
    torch.nn.Linear(num_hidden_units, 1, bias=False)   # c * sigmoid(a + bx)
    # to be 100% equivalent to python&numpy&pytorch-halfway networks above we can't have a bias in last Linear layer
  )

  if bTest:
    # copy weight init from numpy example so it's exact match for debugging:
    a = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    b = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    c = [random.gauss(0.0, 1.0) for _ in range(num_hidden_units)]
    with torch.no_grad():
      for i in range(num_hidden_units):
        model[0].bias[i] = a[i]
        model[0].weight[i, 0] = b[i]
        model[2].weight[0, i] = c[i]

  loss_fn = torch.nn.MSELoss(reduction='sum')
  optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)
  training_data = SimpleDataset(data)
  training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=(not bTest))

  for epoch in range(num_epochs):
    epoch_loss = 0.0
    for i, train_data in enumerate(training_loader):
      # Every data instance is an input + label pair
      x, y = train_data
      # print(f"{x.shape=} {y.shape=}")
      x = torch.reshape(x, (len(x), 1))
      y = torch.reshape(y, (len(y), 1))
      # print(f"{x.shape=} {y.shape=}")
      output_vec = model(x)
      loss = loss_fn(output_vec, y)
      with torch.no_grad():
        epoch_loss += loss.item()
      # backward - compute gradient vectors
      optimizer.zero_grad()
      loss.backward()
      # optimizer step - SGD - update weight vectors
      optimizer.step()
    if epoch == 0 or epoch % 1000 == 999:
      print(f"{epoch=} {epoch_loss=}")

  linear_layer = model[0]
  print(f'a= {linear_layer.bias}')
  print(f'b= {linear_layer.weight}')
  linear_layer = model[2]
  print(f'c= {linear_layer.weight}')
  time_end = time.time()
  time_diff = time_end - time_start
  print(f"Took {time_diff:.3f} seconds {(time_diff/60):.3f} minutes {(time_diff/3600):.3f} hours.")

train_torch_batch(data)