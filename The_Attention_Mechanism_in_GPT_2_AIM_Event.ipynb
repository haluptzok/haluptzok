{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWiGVj6njoDn"
      },
      "source": [
        "# Unsupervised Pre-Training of GPT-Style Model\n",
        "\n",
        "In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n",
        "\n",
        "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n",
        "\n",
        "> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBGw2a0bfT-Y"
      },
      "source": [
        "## Architecture Diagram\n",
        "\n",
        "This is the diagram we'll be using to guide our later intuitions about the model architecture!\n",
        "\n",
        "![image](https://i.imgur.com/WtcoFOo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHi04aEnkKEZ"
      },
      "source": [
        "## Data Selection\n",
        "\n",
        "For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n",
        "\n",
        "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
        "\n",
        "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n",
        "\n",
        "Let's start by grabbing our source repository for the day!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRsEQZy6tgc",
        "outputId": "f10fc37b-f55a-4827-8ece-84f9e3ee5aaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'nanoGPT'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l4CqoEDl7ks"
      },
      "source": [
        "Next, we'll need to grab some dependencies.\n",
        "\n",
        "`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_gepPv1Qdj_",
        "outputId": "09ecd75b-1f05-43bf-e4fd-76e972c73c61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken requests cohere openai -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70hSjXmZmCt3"
      },
      "source": [
        "First things first - let's download our dataset!\n",
        "\n",
        "We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T7qRWArUNiZ5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "current_path = \"/data/shakespeare\"\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(current_path):\n",
        "    os.makedirs(current_path)\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU9BG2CymU-a"
      },
      "source": [
        "Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFnrwKpQPsYh",
        "outputId": "600d41c7-3f68-4c9c-e81d-58714d2217c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmWXE5ctma9Z"
      },
      "source": [
        "We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n",
        "\n",
        "Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLecDiHbogvX"
      },
      "source": [
        "### What is BPE?\n",
        "\n",
        "First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n",
        "\n",
        "The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n",
        "\n",
        "Let's take the following text and break it apart into its word components.\n",
        "\n",
        "\n",
        "```\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "```\n",
        "\n",
        "A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m34NDAGCpiz6"
      },
      "outputs": [],
      "source": [
        "input_text = \"\"\"\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "naive_word_list = input_text.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR8k-2bopqjy"
      },
      "source": [
        "Now we can count our words and get their frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_201bSQpvqD",
        "outputId": "91790512-18ec-47ae-ea18-63145502ac0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab_and_frequencies = defaultdict(int)\n",
        "\n",
        "for word in naive_word_list:\n",
        "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
        "\n",
        "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NckufSxxp-w5"
      },
      "source": [
        "Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BNcjzjDvvKjp"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple, List, Set\n",
        "\n",
        "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
        "  vocab = set()\n",
        "\n",
        "  for word in current_vocab.keys():\n",
        "    for subword in word.split():\n",
        "      vocab.add(subword)\n",
        "\n",
        "  return len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3kCf-WvdBL",
        "outputId": "dbc50b42-0345-4aa2-af9d-98ce769c6043"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_vocabulary_size(vocab_and_frequencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoMq7GhKqf7p"
      },
      "source": [
        "As we can see, there are 36 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGxrHYmftDTr"
      },
      "source": [
        "Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sTwvfTAErQN7"
      },
      "outputs": [],
      "source": [
        "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  pairs = {}\n",
        "\n",
        "  for word, frequency in current_vocab.items():\n",
        "    symbols = word.split()\n",
        "\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i + 1])\n",
        "      current_frequency = pairs.get(pair, 0)\n",
        "      pairs[pair] = current_frequency + frequency\n",
        "\n",
        "  return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FudOaKmYv9-y"
      },
      "outputs": [],
      "source": [
        "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIJfkk7wFYw",
        "outputId": "903ecff7-9de5-45e3-e1c4-63c7a083af5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('t', 'h'), 11),\n",
              " (('i', 'n'), 10),\n",
              " (('r', 'e'), 8),\n",
              " (('h', 'e'), 8),\n",
              " (('a', 't'), 7)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqORqdzwsZ6s"
      },
      "source": [
        "Now that we have the frequent pairs - we can merge those pairs into a single token.\n",
        "\n",
        "Let's see how this process looks in code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "L7ohHm2kshoY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  vocab_out = {}\n",
        "\n",
        "  pattern = re.escape(' '.join(most_common_pair))\n",
        "  replacement = ''.join(most_common_pair)\n",
        "\n",
        "  for word_in in current_vocab:\n",
        "      word_out = re.sub(pattern, replacement, word_in)\n",
        "      vocab_out[word_out] = current_vocab[word_in]\n",
        "\n",
        "  return vocab_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ab760KKuwzZ6"
      },
      "outputs": [],
      "source": [
        "new_vocab_and_frequencies = merge_vocab(\n",
        "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
        "    vocab_and_frequencies\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0XtvLbpxbSx",
        "outputId": "10db54fe-4013-4afd-c084-f8160d84b5e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DPkBzj2u-me"
      },
      "source": [
        "After one merge, we can see that `t h` has been converted to `th`!\n",
        "\n",
        "Let's see how that impacted our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_xegCtxjQf",
        "outputId": "7babce8c-4198-497f-f9f8-2951f5406527"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_vocabulary_size(new_vocab_and_frequencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3M13D60xzZi"
      },
      "source": [
        "We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n",
        "\n",
        "In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BePYCbHly02H"
      },
      "source": [
        "## Training Our Tokenizer\n",
        "\n",
        "Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n",
        "\n",
        "Let's walk through the steps we'll take:\n",
        "\n",
        "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
        "\n",
        "  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n",
        "  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n",
        "\n",
        "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n",
        "\n",
        "  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n",
        "\n",
        "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n",
        "\n",
        "  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n",
        "  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OrztE09OPosB"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Sequence([NFD()])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDqkNNdM1KsD"
      },
      "source": [
        "We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n",
        "\n",
        "Let's use the following:\n",
        "\n",
        "- `\"<s>\"`    : bos_token - beginning of sequence token\n",
        "- `\"</s>\"`   : eos_token - end of sequence token\n",
        "- `\"<pad>\"`  : padding_token - token used to pad sequences\n",
        "- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n",
        "- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n",
        "\n",
        "We're also going to set a target vocabulary of 50,000 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x9iQVhN3P3RN"
      },
      "outputs": [],
      "source": [
        "trainer = BpeTrainer(\n",
        "    vocab_size=50000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\n",
        "      \"<s>\",\n",
        "      \"<pad>\",\n",
        "      \"</s>\",\n",
        "      \"<unk>\",\n",
        "      \"<mask>\"\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ8X9vZe2Fyw"
      },
      "source": [
        "Nothing left to do but point it at our data-source and let it train!\n",
        "\n",
        "We'll use the `.train()` method to accomplish this task.\n",
        "\n",
        "> NOTE: Pay attention to the desired inputs of the `.train()` method.\n",
        "\n",
        "- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LinLHotSP7gv"
      },
      "outputs": [],
      "source": [
        "tokenizer.train(files=[input_file_path], trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2JNYiqB2qKV"
      },
      "source": [
        "Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk6QjDGHQy2K",
        "outputId": "56c30454-38dc-4c9d-dcfd-8044c0239cd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/tokenizer\\\\vocab.json', '/content/tokenizer\\\\merges.txt']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_path = '/content/tokenizer'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOlbggdRFrN",
        "outputId": "cda96241-5159-4b1b-c39d-ea4031141a0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "us1vofdhQ45C"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Bnq7lV2xWo"
      },
      "source": [
        "Let's see how it tokenizes our inputs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dnYnFa3fTRLf"
      },
      "outputs": [],
      "source": [
        "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHY5VufRbBj",
        "outputId": "67d0c201-e096-4061-b992-7e257f0e0783"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hark',\n",
              " ',',\n",
              " 'Ġmy',\n",
              " 'Ġname',\n",
              " 'Ġbe',\n",
              " 'ĠRomeo',\n",
              " '!',\n",
              " 'ĠI',\n",
              " 'Ġam',\n",
              " 'Ġbut',\n",
              " 'Ġa',\n",
              " 'Ġbeautiful',\n",
              " 'Ġsummer',\n",
              " \"'s\",\n",
              " 'Ġday',\n",
              " '!']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentence = tokenizer.tokenize(input_sentence)\n",
        "tokenized_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrWzQQlTU41",
        "outputId": "d0572b81-1633-464a-fff6-b58ae0520d38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[12079, 9, 126, 639, 123, 828, 5, 89, 297, 221, 74, 9115, 3001, 143, 513, 5]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "encoded_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oS6lE-NLRnzk",
        "outputId": "e9b2a346-c8b4-452b-a487-b6f57517bb38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n",
        "decoded_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji3sF-rA21YH"
      },
      "source": [
        "## Tokenizing Dataset\n",
        "\n",
        "Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n",
        "\n",
        "We'll simply encode our training and validation data - and then save them in binary files for later!\n",
        "\n",
        "> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calHML6JPnCU",
        "outputId": "c6153252-b5ea-4f51-f83f-a87414561ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train has 291,284 tokens\n",
            "val has 34,223 tokens\n"
          ]
        }
      ],
      "source": [
        "train_ids = tokenizer.encode(train_data)\n",
        "val_ids = tokenizer.encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nKJ1KqiiPkRh"
      },
      "outputs": [],
      "source": [
        "# export to bin files\n",
        "data_path = \"/data/shakespeare/\"\n",
        "\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFbbvIi7xsgr"
      },
      "source": [
        "Let's look at our first 100 training tokens to see what format they are in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9z7ia8AxqEn",
        "outputId": "e9c282cc-5277-4624-b72c-a495d9c8b90e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  21,  390,  878,   13,   68, 6806,  375,  155, 2503,  624, 2094,\n",
              "          9,  498,  138,  435,   11,   68,   68,   16,   91,   13,   68,\n",
              "         34, 7884,    9,  435,   11,   68,   68,   21,  390,  878,   13,\n",
              "         68,   40,   75,  254,  229, 3780, 1306,  105,  783,  353,  105,\n",
              "       7506,   15,   68,   68,   16,   91,   13,   68,   33,   99, 5792,\n",
              "         11, 3780,   11,   68,   68,   21,  390,  878,   13,   68,   21,\n",
              "        390,    9,  106,  332, 3319, 1179,  147, 3565, 1768,  105,   82,\n",
              "       1008,   11,   68,   68,   16,   91,   13,   68, 7799,  332,  488,\n",
              "          9,  155,  332,  488,   11,   68,   68,   21,  390,  878,   13,\n",
              "         68], dtype=uint16)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ids[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0I3VrRC3XIO"
      },
      "source": [
        "## Training The Model\n",
        "\n",
        "Now that we have our tokenized dataset, let's get to training our model!\n",
        "\n",
        "We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n",
        "\n",
        "First, let's literally jump into the `nanoGPT` repository we cloned earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUU2jaalUdqm",
        "outputId": "b49a6386-68da-4b09-a6f8-0b4ee7490584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\haluptzok\\nanoGPT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\torch\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd nanoGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13p1e8sa3k0V"
      },
      "source": [
        "We'll do some critical imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "weNR37BwUYNg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from the local repo\n",
        "from model import GPTConfig, GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY_vWZG-3uM-"
      },
      "source": [
        "### Hyper-Parameters\n",
        "\n",
        "We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OykCjVQK5EX-"
      },
      "source": [
        "#### I/O\n",
        "\n",
        "- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "viM3qlWt5PVS"
      },
      "outputs": [],
      "source": [
        "out_dir = 'out'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5iwwrNL5H4C"
      },
      "source": [
        "#### Initialization\n",
        "\n",
        "Since we're training from scratch, we'll use `init_from = 'scratch'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OK1z2m3C312T"
      },
      "outputs": [],
      "source": [
        "init_from = 'scratch'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YlolKOj4_dE"
      },
      "source": [
        "#### Eval and Logging\n",
        "\n",
        "- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n",
        "- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n",
        "- `eval_iters` - this is how *many* iterations we want to evaluate for.\n",
        "- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n",
        "- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MbFN5Ltq4_mo"
      },
      "outputs": [],
      "source": [
        "eval_interval = 250\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "eval_only = False\n",
        "always_save_checkpoint = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a488zaF_4zQk"
      },
      "source": [
        "#### Dataset\n",
        "\n",
        "We can set our dataset here - we'll use the one we created earlier!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_QC7vWXC40Hp"
      },
      "outputs": [],
      "source": [
        "dataset = 'shakespeare'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP9rBgGc426Q"
      },
      "source": [
        "#### Typical Hyper-Parameters\n",
        "\n",
        "- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n",
        "- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n",
        "- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "EM_ybLPP43Pd"
      },
      "outputs": [],
      "source": [
        "gradient_accumulation_steps = 1\n",
        "batch_size = 16\n",
        "block_size = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ-8bDIY45GS"
      },
      "source": [
        "#### Model Architecture\n",
        "\n",
        "- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n",
        "- `n_head` - this is the number of attention heads in each decoder layer!\n",
        "- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook.\n",
        "- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n",
        "- `bias` - wether or not to use bias inside the LayerNorm/Linear layers.\n",
        "\n",
        "> NOTE: You need to ensure your `n_embd` is cleanly divided by your `n_head`. That is to say:\n",
        ">\n",
        "> `n_embd % n_head == 0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gMyyDBxB6k4H"
      },
      "outputs": [],
      "source": [
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 516\n",
        "dropout = 0.2\n",
        "bias = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NWDTaAz7gwh"
      },
      "source": [
        "#### Optimizer Hyper-Parameters\n",
        "\n",
        "Basic Optimizer Hyper-Parameters:\n",
        "\n",
        "- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n",
        "- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n",
        "\n",
        "Learning Rate Decay Settings:\n",
        "\n",
        "- `decay_lr` - set decay flag\n",
        "- `weight_Decay` - how much to decay lr by\n",
        "- `lr_decay_iters` - should be set to ~max_iters.\n",
        "- `min_lr` - the minimum lr, should be ~ lr / 10\n",
        "\n",
        "Clipping and Warmup:\n",
        "\n",
        "- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n",
        "- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n",
        "\n",
        "> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qe-669jwUptI"
      },
      "outputs": [],
      "source": [
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5_000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "\n",
        "# lr decay settings\n",
        "decay_lr = True\n",
        "weight_decay = 1e-1\n",
        "lr_decay_iters = 5_000\n",
        "min_lr = 1e-4\n",
        "\n",
        "# clipping and warmup\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucldc4mz9yeT"
      },
      "source": [
        "These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiGlMOp8Nux",
        "outputId": "49097c98-cbc6-4bd8-a30d-bbefe3e10a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 8,192\n"
          ]
        }
      ],
      "source": [
        "backend = 'nccl'\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = True\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKmdfbye-BNf"
      },
      "source": [
        "### Torch Settings\n",
        "\n",
        "We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n",
        "\n",
        "Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yh34QGD6VARU"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKeNwYaZ-Zoc"
      },
      "source": [
        "### Dataloader\n",
        "\n",
        "This block will:\n",
        "\n",
        "1. Set the data path\n",
        "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
        "3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tOjaPyJpVEgx"
      },
      "outputs": [],
      "source": [
        "data_dir = os.path.join('/data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z7vMU34yRbq"
      },
      "source": [
        "Let's look at what an example of our batches would look like.\n",
        "\n",
        "To remind ourselves:\n",
        "\n",
        "- `train_data` - has ~2.9 million entries\n",
        "- `block_size` - is 512\n",
        "- `batch_size` - is 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9_-Y5RZ-yX2a"
      },
      "outputs": [],
      "source": [
        "ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDxXph4yh2g",
        "outputId": "27fd09cf-8f21-4013-80f3-d55c00f9d88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our randomly selected indices were: tensor([ 99775, 155569, 263696,  32920,  52919, 231541, 153767, 229238, 136782,\n",
            "        263618,  39008,  14208,  39429, 189430, 194466,  76798])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Our randomly selected indices were: {ix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAzHJH-kzK5E",
        "outputId": "40289227-7659-42d0-edb0-008961057d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first 10 elements of `x` at the first randomly selected index is:\n",
            "tensor([   68,    16,    83,  2360, 19951,   118,   174,  1282,     9,    68])\n"
          ]
        }
      ],
      "source": [
        "print(f\"The first 10 elements of `x` at the first randomly selected index is:\\n{x[0][:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaF6v8ezkWn",
        "outputId": "bed6fcf8-0906-4720-809b-3b199ce60d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first 10 elements of `y` at the first randomly selected index is:\n",
            "tensor([   16,    83,  2360, 19951,   118,   174,  1282,     9,    68,    16])\n"
          ]
        }
      ],
      "source": [
        "print(f\"The first 10 elements of `y` at the first randomly selected index is:\\n{y[0][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N62oDfdWy0XJ"
      },
      "source": [
        "So the first component selects a random index from our training data (accounting for our block size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbDlW-68_atH"
      },
      "source": [
        "### Simple Initialization of Model\n",
        "\n",
        "Here we init our number of iterations as 0, and our best val loss as a very high number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6hsepdVBVzQU"
      },
      "outputs": [],
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Uj9qBI_vXc"
      },
      "source": [
        "Obtain our vocab size from our trained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m53DcCdFV0_a",
        "outputId": "4e585d96-f35c-4995-c809-32a79ccec7a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20101"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = tokenizer.vocab_size\n",
        "meta_vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7bcNelYARmD"
      },
      "source": [
        "Create our model args dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "JfIWEbanV7ZS"
      },
      "outputs": [],
      "source": [
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WWcbkiCAUI2"
      },
      "source": [
        "Instantiate our model with the provided `model_args`.\n",
        "\n",
        "These are derived from the hyper-parameters we set above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xly4iA0V-vF",
        "outputId": "dbfcf75a-249c-4162-b07d-141d555c7483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 29.55M\n"
          ]
        }
      ],
      "source": [
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpViOsxLAl6p"
      },
      "source": [
        "There we go! If you used the default values - you should have a model with 29.55M parameters!\n",
        "\n",
        "Let's set our block_size to the correct size as determined in our configuration steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "TrEawNxdWRhm"
      },
      "outputs": [],
      "source": [
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRgguPLKAuZ5"
      },
      "source": [
        "Now we can look at our model in all its glory!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaE3KSTnAtJs",
        "outputId": "76862c66-60ec-40c4-c496-f82160ee12a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(20101, 516)\n",
              "    (wpe): Embedding(512, 516)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n",
              "          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=516, out_features=20101, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiTlDpQQaq0W"
      },
      "source": [
        "## Model Architecture Breakdown\n",
        "\n",
        "Now that we've built our model - let's look at each component and see how it works!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwAK0lfaxtY"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "We've already talked about tokenization - so let's expand on it a bit here:\n",
        "\n",
        "![image](https://i.imgur.com/oYuAayM.png)\n",
        "\n",
        "As you can see - our sentences are simply converted from a string to a sequence of numeric values - each which represents a word or subword!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_4mk2yKbFF4"
      },
      "source": [
        "### Embedding Layers\n",
        "\n",
        "The first step will be do convert our tokenized sequence of inputs into an embedding vector. This allows use to understand a rich amount of information about input sequences and their semantic meanings.\n",
        "\n",
        "As the embedding layer will be training along side the rest of the model - it will allow us to have an excellent vector-representation of the tokens in our dataset.\n",
        "\n",
        "Let's see how it looks in code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntf8i9EabKwD"
      },
      "source": [
        "```python\n",
        "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh9pH_4dbWan"
      },
      "source": [
        "Now let's see a visual example!\n",
        "\n",
        "![image](https://i.imgur.com/Q8fiuw2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNPEiOejbcBt"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "We need to impart information about where each token is in the sequence, but we aren't using any recurrence or convolutions - the easiest way to encode positional information is to inject positional information into our input embeddings.\n",
        "\n",
        "We can use two different methods to obtain our positional encodings - either a learned positional encoding, or a calculated positional encoding.\n",
        "\n",
        "Let's look at the code from the example used in `nanoGPT`, and also an example of code that uses a calculated positional encoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaJQVo0ibujN"
      },
      "source": [
        "```python\n",
        "wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QEmKK8Dbybh"
      },
      "source": [
        "A familiar face! In this case, our positional encodings are quite similar to our embeddings layer - but we're setting the dimensionality to care about the `block_size` (context window) as opposed to the `vocab_size`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRFQN34qfnZe"
      },
      "source": [
        "Another method we could use is the calculated positional embeddings - which could be implemented like so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCrtpp6pfsgC"
      },
      "source": [
        "```python\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float, verbose=False) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.verbose=verbose\n",
        "\n",
        "    positional_embeddings = torch.zeros(seq_len, d_model)\n",
        "    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings = positional_embeddings.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('positional_embeddings', positional_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    if self.verbose:\n",
        "      print(f\"Positional Encodings (1st 5 elements): {x[0, :5]}\")\n",
        "    return self.dropout(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eODsnnADfxGr"
      },
      "source": [
        "In this case, we're implementing this set of equations from \"Attention is All You Need\"!\n",
        "\n",
        "![image](https://i.imgur.com/UhAJ0H0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Wh1cAxlY1b"
      },
      "source": [
        "Let's look at this process visually to better understand:\n",
        "\n",
        "![image](https://i.imgur.com/JKwbESf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Zfr5tAgF33"
      },
      "source": [
        "### Attention Mechanism\n",
        "\n",
        "Now we get to the \"meat\" of the model - the scaled dot-product attention mechanism!\n",
        "\n",
        "Let's first take a look at how it's implemented in code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5zI82M8gTfL"
      },
      "source": [
        "```python\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a7c1OnbgdW5"
      },
      "source": [
        "Let's discuss a few key components and describe how they align with the attention mechanism outlined in the paper!\n",
        "\n",
        "First:\n",
        "\n",
        "```python\n",
        "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "```\n",
        "\n",
        "This is the where we find our \"scaled dot-product\" attention scores!\n",
        "\n",
        "Second:\n",
        "\n",
        "```python\n",
        "att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "```\n",
        "\n",
        "This is where we're applying our \"causal mask\" that ensures we're only ever looking back, and can't look forward past our current token.\n",
        "\n",
        "Third:\n",
        "\n",
        "```python\n",
        "att = F.softmax(att, dim=-1)\n",
        "```\n",
        "\n",
        "This is where we're computing the soft-max of our attention scores.\n",
        "\n",
        "Fourth:\n",
        "\n",
        "```python\n",
        "y = att @ v\n",
        "```\n",
        "\n",
        "This is where we're doing the last MatMul between our computed soft-max attention scores and our value vector!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM2VtkhLkfaZ"
      },
      "source": [
        "![image](https://i.imgur.com/lJOWXcE.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOkvFxqClDDo"
      },
      "source": [
        "## Back to Initializing Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzoEY6gcBOSp"
      },
      "source": [
        "We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "BNUThRt4WT5H"
      },
      "outputs": [],
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zs5Hcf9BBUD"
      },
      "source": [
        "Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesGeUnoWViL",
        "outputId": "d05459ae-4a9d-44dc-b0c8-a1d597089883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 29,806,740 parameters\n",
            "num non-decayed parameter tensors: 13, with 6,708 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay,\n",
        "    learning_rate,\n",
        "    (beta1, beta2),\n",
        "    device_type\n",
        ")\n",
        "\n",
        "checkpoint = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF5YWJoKB4og"
      },
      "source": [
        "Now we can compile our model!\n",
        "\n",
        "If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n",
        "\n",
        "Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FNU0T0WXdI",
        "outputId": "bacfa39c-d7b9-4055-b430-e059ad3f3e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ],
      "source": [
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    # model = torch.compile(model) # requires PyTorch 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6lRcVsZCXRO"
      },
      "source": [
        "We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n",
        "\n",
        "You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "lUB5zVLVWbhM"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLsOpaACDDkF"
      },
      "source": [
        "### Creating our LR Scheduler\n",
        "\n",
        "Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n",
        "\n",
        "We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n",
        "\n",
        "![img](https://i.imgur.com/KoFEl0b.png)\n",
        "\n",
        "There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "7-mNpWBSWdHh"
      },
      "outputs": [],
      "source": [
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqFePCZmE1Lq"
      },
      "source": [
        "We need to set some specific values in our env to allow training in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nDL6s4YT6E",
        "outputId": "9e793721-22c8-4a94-aea2-dde66456fa6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'export' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'export' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'export' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'ldconfig' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhqmxeo0Eg0Z"
      },
      "source": [
        "## The Training Loop\n",
        "\n",
        "Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHbyEapRWmpc",
        "outputId": "2c041fdb-0571-4ae7-9808-7ad31b51c6cd"
      },
      "outputs": [],
      "source": [
        "X, Y = get_batch('train')\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model\n",
        "running_mfu = -1.0 # model flops utilization\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2J5JlRxFJOM"
      },
      "source": [
        "## Generating Outputs with our New Model\n",
        "\n",
        "Now we can leverage the `sample.py` file to generate outputs from our model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo_QP1ITFfX2"
      },
      "source": [
        "### Generation Set Up and Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vftqU9LheEK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRB3j7iiNkl",
        "outputId": "a2046037-51b6-4187-f49a-52e8861b901b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 29.55M\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1YAy8DriVZZ"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoB-5ZuLicAT"
      },
      "outputs": [],
      "source": [
        "enc = tokenizer\n",
        "encode = lambda s: enc.encode(s)\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkTQ9wo7FjYU"
      },
      "source": [
        "### Generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmTcaHCjii5l",
        "outputId": "47f66833-fc35-495d-a3c0-7cff16718b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "To the crown me in this golden crown,\n",
            "Command those few, revenues and moveables\n",
            "Whereof the last I did stand possess'd.\n",
            "\n",
            "KING RICHARD II:\n",
            "How well thou tell'st thou see that,\n",
            "Give me the king of Lancaster and my care?\n",
            "Dear brother, and my inward soul:\n",
            "Which finds Edward is lost for that,\n",
            "As Harry Duke of York, and grief.\n",
            "\n",
            "KING RICHARD II:\n",
            "But not the one:\n",
            "My lord, that I am;\n",
            "I for I not king, and all your comfort;\n",
            "With nothing had not been as for.\n",
            "\n",
            "Since I was? I am a dear;\n",
            "\n",
            "BUSHY:\n",
            "'Tis nothing but subjects,\n",
            "As little joy that I do not so my love.\n",
            "\n",
            "QUEEN:\n",
            "'Tis nothing less: conceit is still derived\n",
            "From some forefather grief; mine is not so,\n",
            "For nothing had begot my something grief;\n",
            "Or something hath the nothing that I grieve:\n",
            "'Tis in reversion that I do possess;\n",
            "But what it is, that is not yet known; what\n",
            "I cannot name; 'tis nameless woe, I wot.\n",
            "\n",
            "GREEN:\n",
            "God save your majesty! and well met, gentlemen:\n",
            "I hope the king is not yet shipp'd for Ireland.\n",
            "\n",
            "QUEEN:\n",
            "Why hopest thou so? 'tis better hope he is;\n",
            "For his designs crave haste, his haste good hope:\n",
            "Then wherefore dost thou hope he is not shipp'd?\n",
            "\n",
            "GREEN:\n",
            "That he, our hope, might have retired his power,\n",
            "And driven into despair an enemy's hope,\n",
            "Who strongly hath set footing in this land:\n",
            "The banish'd Bolingbroke repeals himself,\n",
            "And with uplifted arms is safe arrived\n",
            "At Ravenspurgh.\n",
            "\n",
            "QUEEN:\n",
            "Now God in heaven forbid!\n",
            "\n",
            "GREEN:\n",
            "Ah, madam, 'tis too true: and that is worse,\n",
            "The Lord Northumberland, his son young Henry Percy,\n",
            "The Lords of Ross, Beaumond, and Willoughby,\n",
            "With all their powerful friends, are fled to him.\n",
            "\n",
            "BUSHY:\n",
            "Why have you not proclaim'd Northumberland\n",
            "And all the rest revolted faction traitors?\n",
            "\n",
            "GREEN:\n",
            "We have: whereupon the Earl of Worcester\n",
            "---------------\n",
            "\n",
            "JULIET:\n",
            "Yea, wherefore?\n",
            "\n",
            "Nurse, I am here.\n",
            "\n",
            "JULIET:\n",
            "Well, come, thou hast comforted me marvellous much.\n",
            "Go in: and tell my lady I am gone,\n",
            "Having displeased my father, to Laurence' cell,\n",
            "To make confession and to be absolved.\n",
            "\n",
            "Nurse:\n",
            "Marry, I will; and this is wisely done.\n",
            "\n",
            "JULIET:\n",
            "Ancient damnation! O most wicked fiend!\n",
            "Is it more sin to wish me thus forsworn,\n",
            "Or to dispraise my lord with that same tongue\n",
            "Which she hath praised him with above compare\n",
            "So many thousand times? Go, counsellor;\n",
            "Thou and my bosom henceforth shall be twain.\n",
            "I'll to the friar, to know his remedy:\n",
            "If all else fail, myself have power to die.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "On Thursday, sir? the time is very short.\n",
            "\n",
            "PARIS:\n",
            "Of I am nothing slow to slack his haste.\n",
            "\n",
            "JULIET:\n",
            "You say you do not know the lady's mind:\n",
            "Uneven is the course, I like it not.\n",
            "\n",
            "PARIS:\n",
            "Immoderately she weeps for Tybalt's death,\n",
            "And therefore have I little talk'd of love;\n",
            "For Venus smiles not in a house of tears.\n",
            "Now, sir, her father counts it dangerous\n",
            "That she doth give her sorrow so much sway,\n",
            "And in his wisdom hastes our marriage,\n",
            "To stop the inundation of her tears;\n",
            "Which, too much minded by herself alone,\n",
            "May be put from her by society:\n",
            "Now do you know the reason of this haste.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "PARIS:\n",
            "Happily met, my lady and my wife!\n",
            "\n",
            "JULIET:\n",
            "That may be, sir, when I may be a wife.\n",
            "\n",
            "PARIS:\n",
            "That may be must be must be must be, on Thursday next.\n",
            "\n",
            "JULIET:\n",
            "What must be shall be shall be.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "That's a certain text.\n",
            "\n",
            "PARIS:\n",
            "Come you to make confession to this father?\n",
            "\n",
            "JULIET:\n",
            "To answer that, I should confess to you.\n",
            "\n",
            "PARIS:\n",
            "Do not\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "The image of medicine\n",
            "But only he so seek.\n",
            "\n",
            "CLAUDIO:\n",
            "ISABELLA:\n",
            "O, but yet in heaven,\n",
            "I am the sin you consenting to't,\n",
            "Would bark your honour from that trunk you bear,\n",
            "And leave you naked.\n",
            "\n",
            "CLAUDIO:\n",
            "Let me know the point.\n",
            "\n",
            "ISABELLA:\n",
            "O, I do fear thee, Claudio; and I quake,\n",
            "Lest thou a feverous life shouldst entertain,\n",
            "And six or seven winters more respect\n",
            "Than a perpetual honour. Darest thou die?\n",
            "The sense of death is most in apprehension;\n",
            "And the poor beetle, that we tread upon,\n",
            "In corporal sufferance finds a pang as great\n",
            "As when a giant dies.\n",
            "\n",
            "CLAUDIO:\n",
            "Why give you me this shame?\n",
            "Think you I can a resolution fetch\n",
            "From flowery tenderness? If I must die,\n",
            "I will encounter darkness as a bride,\n",
            "And hug it in mine arms.\n",
            "\n",
            "ISABELLA:\n",
            "There spake my brother; there my father's grave\n",
            "Did utter forth a voice. Yes, thou must die:\n",
            "Thou art too noble to conserve a life\n",
            "In base appliances. This outward-sainted deputy,\n",
            "Whose settled visage and deliberate word\n",
            "Nips youth i' the head and follies doth emmew\n",
            "As falcon doth the fowl, is yet a devil\n",
            "His filth within being cast, he would appear\n",
            "A pond as deep as deep as hell.\n",
            "\n",
            "CLAUDIO:\n",
            "The prenzie Angelo!\n",
            "\n",
            "ISABELLA:\n",
            "O, 'tis the cunning livery of hell,\n",
            "The damned'st body to invest and cover\n",
            "In prenzie guards! Dost thou think, Claudio?\n",
            "If I would yield him my virginity,\n",
            "Thou mightst be freed.\n",
            "\n",
            "CLAUDIO:\n",
            "O heavens! it cannot be. This night's the time\n",
            "That I should do what I abhor to name,\n",
            "Or else thou diest to-morrow.\n",
            "\n",
            "ISABELLA:\n",
            "Thou shalt not do't.\n",
            "\n",
            "CLAUDIO:\n",
            "O, were it but my life,\n",
            "I'ld throw it down for your deliverance\n",
            "As frankly as a pin.\n",
            "\n",
            "ISABELLA:\n",
            "Thanks, dear Isabel.\n",
            "\n",
            "CLAUDIO:\n",
            "Be ready, dear Isabel.\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "KING RICHARD III:\n",
            "I did love her fair soul.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "But she, your mother.\n",
            "\n",
            "KING RICHARD III:\n",
            "What, I learn of bleeding- bleeding-hearts; thereon engrave\n",
            "Edward and York; then haply she will weep:\n",
            "Therefore present to her--as sometime Margaret\n",
            "Did to thy father, steep'd in Rutland's blood,--\n",
            "A handkerchief; which, say to her, did drain\n",
            "The purple sap from her sweet brother's body\n",
            "And bid her dry her dry her dry her weeping eyes therewith.\n",
            "If this inducement force her not to love,\n",
            "Send her a story of thy noble acts;\n",
            "Tell her thou madest away her uncle Clarence, and,\n",
            "Her uncle Rivers; yea, and, for her sake,\n",
            "Madest quick conveyance with her good aunt Anne.\n",
            "\n",
            "KING RICHARD III:\n",
            "Come, come, you mock me; this is not the way\n",
            "To win our daughter.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "There is no other way\n",
            "Unless thou couldst put on some other shape,\n",
            "And not be Richard that hath done all this.\n",
            "\n",
            "KING RICHARD III:\n",
            "Say that I did all this for love of her.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Nay, then indeed she cannot choose but hate thee,\n",
            "Having bought love with such a bloody spoil.\n",
            "\n",
            "KING RICHARD III:\n",
            "Look, what is done cannot be now amended:\n",
            "Men shall deal unadvisedly sometimes,\n",
            "Which after hours give leisure to repent.\n",
            "If I did take the kingdom from your sons,\n",
            "To make amends, Ill give it to your daughter.\n",
            "If I have kill'd the issue of your increase,\n",
            "To quicken your increase, I will beget\n",
            "Mine issue of your blood upon your daughter\n",
            "A grandam's name is little less in love\n",
            "Than is the doting title of a mother;\n",
            "They are as children but one step below,\n",
            "Endured of your very blood.\n",
            "Your children were vexation to your youth,\n",
            "But mine shall be a comfort to your age.\n",
            "The loss you have is but a son being king,\n",
            "And by that loss your daughter is made queen.\n",
            "I cannot make you what amends I would,\n",
            "Therefore accept such kindness as I can.\n",
            "Dorset your son, that with\n",
            "---------------\n",
            "\n",
            "And say there an hour before his last,\n",
            "That I slew him to death, he slew thy heart,\n",
            "Did I 'twas Rutland;\n",
            "But 'twas I that Rutland; which so his sake:\n",
            "Thou hadst but Edward;\n",
            "And, that ill- Rutland; which this is just,\n",
            "Didst thou revenged on him that ill-hound that doth hunt us all to death:\n",
            "That dog, to death, that makes this to death,\n",
            "To worry lambs and lap their gentle blood,\n",
            "That foul defacer of God's handiwork,\n",
            "That excellent grand tyrant of the earth,\n",
            "That reigns in galled eyes of weeping souls,\n",
            "Thy womb let loose, to chase us to our graves.\n",
            "O upright, just, just, and true-disposing God,\n",
            "How do I thank thee, that this carnal cur\n",
            "Preys on the issue of his mother's body,\n",
            "And makes her pew-fellow with others' moan!\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "O Harry's wife, triumph not in my woes!\n",
            "God witness with me, I have wept for thine.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Bear with me; I am hungry for revenge,\n",
            "And now I cloy me with beholding it.\n",
            "Thy Edward he is dead, that stabb'd my Edward dead, to quit my Edward;\n",
            "Thy other Edward dead, to quit my Edward;\n",
            "Young York he is but boot, because both they\n",
            "Match not the high perfection of my loss:\n",
            "Thy Clarence he is dead that kill'd my Edward;\n",
            "And the beholders of this tragic play,\n",
            "The adulterate Hastings, Rivers, Vaughan, Grey,\n",
            "Untimely smother'd in their dusky graves.\n",
            "Richard yet lives, hell's black intelligencer,\n",
            "Only reserved their factor, at hand, to buy souls\n",
            "And send them thither: but at hand, at hand, at hand,\n",
            "Ensues his piteous and unpitied end:\n",
            "Earth gapes, hell burns, fiends roar, saints pray.\n",
            "To have him suddenly convey'd away.\n",
            "Cancel his bond of life, dear God, I prey,\n",
            "That I may live to say, The dog is dead!\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O, thou didst prophesy the time would come\n",
            "That I should wish for thee to help me curse\n",
            "That bottled spider,\n",
            "---------------\n",
            "\n",
            "If you are not, and make\n",
            "My life did service indeed, for mine.\n",
            "\n",
            "KING RICHARD III:\n",
            "Your own is bankrupt so: what you brings the worst.\n",
            "\n",
            "STANLEY:\n",
            "My lord, Buckingham,\n",
            "And they indeed, to please.\n",
            "\n",
            "KING RICHARD III:\n",
            "Hoyday, Buckingham, Buckingham, as you guess?\n",
            "\n",
            "STANLEY:\n",
            "Unless for that he comes to be told.\n",
            "\n",
            "KING RICHARD III:\n",
            "Hoyday, his son George-liver'd runagate, what doth he there?\n",
            "\n",
            "STANLEY:\n",
            "Well, mighty sovereign, but by guess?\n",
            "STANLEY:\n",
            "Stirr'd up by Dorset, Buckingham, and Ely,\n",
            "He makes for England, there to claim the crown.\n",
            "\n",
            "KING RICHARD III:\n",
            "Is the chair empty? is the sword unsway'd?\n",
            "Is the king dead? the empire unpossess'd?\n",
            "What heir of York is there alive but great York's king but great York's heir?\n",
            "And who is England's king but great York's heir?\n",
            "Then, tell me, what doth he upon the sea?\n",
            "\n",
            "STANLEY:\n",
            "Unless for that, my liege, I cannot guess.\n",
            "\n",
            "KING RICHARD III:\n",
            "Unless for that he comes to be your liege,\n",
            "You cannot guess wherefore the Welshman comes.\n",
            "Thou wilt revolt, and fly to him, I fear.\n",
            "\n",
            "STANLEY:\n",
            "No, mighty liege; therefore mistrust me not.\n",
            "\n",
            "KING RICHARD III:\n",
            "Where is thy power, then, to beat him back?\n",
            "Where are thy tenants and thy followers?\n",
            "Are they not now upon the western shore.\n",
            "Safe-conducting the rebels from their ships!\n",
            "\n",
            "STANLEY:\n",
            "No, my good lord, my friends are in the north.\n",
            "\n",
            "KING RICHARD III:\n",
            "Cold friends to Richard: what do they in the north,\n",
            "When they should serve their sovereign in the west?\n",
            "\n",
            "STANLEY:\n",
            "They have not been commanded, mighty sovereign:\n",
            "Please it your majesty to give me leave,\n",
            "I'll muster up my friends, and meet your grace\n",
            "Where and what time your majesty shall please.\n",
            "\n",
            "KING RICHARD III:\n",
            "Ay, ay. thou wouldst be gone to join with Richmond:\n",
            "I will not trust you, sir.\n",
            "\n",
            "STANLEY:\n",
            "Most mighty\n",
            "---------------\n",
            "\n",
            "The slave, when we ordained festival,\n",
            "Turn back to black funeral bell;\n",
            "Our wedding cheer to melancholy bells,\n",
            "Our wedding cheer to a sad burial feast,\n",
            "Our solemn hymns to sullen dirges change,\n",
            "Our bridal flowers serve for a buried corse,\n",
            "And all things change them to the contrary.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Sir, go you in; and, madam, go with him;\n",
            "And go, Sir Paris; every one prepare\n",
            "To follow this fair corse unto her grave:\n",
            "The heavens do lour upon you for some ill;\n",
            "Move them no more by crossing their high will.\n",
            "\n",
            "First Musician:\n",
            "Faith, we may put up our pipes, and be gone.\n",
            "\n",
            "Nurse:\n",
            "Honest goodfellows, ah, put up;\n",
            "For, well you know, this is a pitiful case.\n",
            "First Musician:\n",
            "Ay, by my troth, the case may be amended.\n",
            "\n",
            "PETER:\n",
            "Musicians, O, musicians, 'Heart's ease, Heart's\n",
            "ease:' O, an you will have me live, play 'Heart's ease.'\n",
            "\n",
            "First Musician:\n",
            "Why 'Heart's ease?'\n",
            "\n",
            "PETER:\n",
            "O, musicians, because my heart itself plays 'My\n",
            "heart is full of woe:' O, play me some merry dump,\n",
            "to comfort me.\n",
            "\n",
            "First Musician:\n",
            "Not a dump we; 'tis no time to play now?\n",
            "\n",
            "PETER:\n",
            "You will not, then?\n",
            "\n",
            "First Musician:\n",
            "No.\n",
            "\n",
            "PETER:\n",
            "I will then give it you soundly.\n",
            "\n",
            "First Musician:\n",
            "What will you give us?\n",
            "\n",
            "PETER:\n",
            "No money, on my faith, but the gleek;\n",
            "I will give you the minstrel.\n",
            "\n",
            "First Musician:\n",
            "Then I will give you the serving-creature.\n",
            "\n",
            "PETER:\n",
            "Then will I lay the serving-creature's dagger on\n",
            "your pate. I will carry no crotchets: I'll re you,\n",
            "I'll fa you; do you note me?\n",
            "\n",
            "First Musician:\n",
            "An you re us and fa us, you note us.\n",
            "\n",
            "Second Musician:\n",
            "Pray you, put up your dagger, and put out your wit.\n",
            "\n",
            "PETER:\n",
            "Then have at you with my wit\n",
            "---------------\n",
            "\n",
            "I could make them that, they say so high,\n",
            "As those should have cause to bear them take hold their hats.\n",
            "But, my lord, let us away.\n",
            "\n",
            "HASTINGS:\n",
            "Go on before; I'll talk.\n",
            "How now, sirrah! how goes the world with thee?\n",
            "\n",
            "Pursuivant:\n",
            "The better that your lordship please to ask.\n",
            "\n",
            "HASTINGS:\n",
            "I tell thee, man, 'tis better with me now\n",
            "Than when I met thee last where now we meet:\n",
            "Then was I going prisoner to the Tower,\n",
            "By the suggestion of the queen's allies;\n",
            "But now, I tell thee--keep it to thyself--\n",
            "This day those enemies are put to death,\n",
            "And I in better state than e'er I was.\n",
            "\n",
            "Pursuivant:\n",
            "God hold it, to your honour's good content!\n",
            "\n",
            "HASTINGS:\n",
            "Gramercy, fellow: there, drink that for me.\n",
            "\n",
            "Pursuivant:\n",
            "God save your lordship!\n",
            "\n",
            "Priest:\n",
            "Well met, my lord; I am glad to see your honour.\n",
            "HASTINGS:\n",
            "I thank thee, good Sir John, with all my heart.\n",
            "I am in your last exercise;\n",
            "Come the next Sabbath, and I will content you.\n",
            "BUCKINGHAM:\n",
            "What, talking with a priest, lord chamberlain?\n",
            "Your friends at Pomfret, they do need the priest;\n",
            "Your honour hath no shriving work in hand.\n",
            "\n",
            "HASTINGS:\n",
            "Good faith, and when I met this holy man,\n",
            "Those men you talk of came into my mind.\n",
            "What, go you toward the Tower?\n",
            "\n",
            "BUCKINGHAM:\n",
            "I do, my lord; but long I shall not stay\n",
            "I shall return before your lordship thence.\n",
            "\n",
            "HASTINGS:\n",
            "'Tis like enough, for I stay dinner there.\n",
            "\n",
            "BUCKINGHAM:\n",
            "HASTINGS:\n",
            "I'll wait upon your lordship.\n",
            "\n",
            "RATCLIFF:\n",
            "Come, bring forth the prisoners.\n",
            "\n",
            "RIVERS:\n",
            "Sir Richard Ratcliff, let me tell thee this:\n",
            "To-day shalt thou behold a subject die\n",
            "For truth, for duty, and for loyalty.\n",
            "\n",
            "GREY:\n",
            "God keep the prince from all the pack of you!\n",
            "A knot you are of damned blood-suckers!\n",
            "---------------\n",
            "\n",
            "LEONTES:\n",
            "Hermione was it, not so.\n",
            "\n",
            "PAULINA:\n",
            "So much more, my good,\n",
            "Not she might have thus she lived now.\n",
            "\n",
            "PAULINA:\n",
            "So much the more such as it is\n",
            "O, my good comfort, as it is\n",
            "In such life, as it is\n",
            "But one of himself, as you.\n",
            "\n",
            "LEONTES:\n",
            "If I am none, I, my lord and graced\n",
            "You knew of those\n",
            "Would have him wed again.\n",
            "\n",
            "PAULINA:\n",
            "As\n",
            "If I had thought no more benefit and graced\n",
            "You break a perpetual all, nor the remembrance\n",
            "Of his most sovereign name; consider little\n",
            "What dangers, by his highness' fail of issue,\n",
            "May drop upon his kingdom and devour\n",
            "Incertain lookers on. What were more holy\n",
            "Than to rejoice the former queen is well?\n",
            "What holier than, for royalty's repair,\n",
            "For present comfort and for future good,\n",
            "To bless the bed of majesty again\n",
            "With a sweet fellow to't?\n",
            "\n",
            "PAULINA:\n",
            "There is none worthy,\n",
            "Respecting her that's gone. Besides, the gods\n",
            "Will have fulfill'd their secret purposes;\n",
            "For has not the divine Apollo said,\n",
            "Is't not the tenor of his oracle,\n",
            "That King Leontes shall not have an heir\n",
            "Till his lost child be found? which that it shall,\n",
            "Is all as monstrous to our human reason\n",
            "As my Antigonus to break his grave\n",
            "As my Antigonus to break his grave\n",
            "Did perish with the infant. 'Tis your counsel\n",
            "My lord should to the heavens be contrary,\n",
            "Oppose against their wills.\n",
            "Care not for issue;\n",
            "The crown will find an heir: great Alexander\n",
            "Left his to the worthiest; so his successor\n",
            "Was like to be the best.\n",
            "\n",
            "LEONTES:\n",
            "Good Paulina,\n",
            "Who hast the memory of Hermione,\n",
            "I know, in honour, O, that ever I\n",
            "Had squared me to thy counsel! then, even now,\n",
            "I might have look'd upon my queen's full eyes,\n",
            "Have taken treasure from her lips--\n",
            "\n",
            "PAULINA:\n",
            "And left them\n",
            "More rich for what they yielded.\n",
            "\n",
            "LEONTES:\n",
            "Thou speak'st truth.\n",
            "---------------\n",
            "\n",
            "Not to be so much as a man; but it is as\n",
            "y. But, Camillo, are to be his departure, are to be, as you\n",
            "ne of the king's, are ignorant:\n",
            "May not as you not honest\n",
            "Your queen, as you are well enjoy,\n",
            "Or to you and so,\n",
            "Would fetch me, as I think honourable:\n",
            "I have loved you, and so craftily;\n",
            "But graciously to know I am no better.\n",
            "\n",
            "LEONTES:\n",
            "By this we speak I am not\n",
            "For this seems so, and in that,\n",
            "But graciously to know I am no better.\n",
            "\n",
            "ANTIGONUS:\n",
            "A gross is not, I am no other, I'll speak I am no better.\n",
            "\n",
            "LEONTES:\n",
            "Force:\n",
            "I dare not:\n",
            "But yet I, you that, and I shall not\n",
            "As this which I'll no better\n",
            "Than you.\n",
            "\n",
            "First Lord:\n",
            "I' the oracle, my queen and more it must not\n",
            "PAULINA:\n",
            "The keeper of the prison,\n",
            "What's the prison is gone?\n",
            "\n",
            "LEONTES:\n",
            "Apollo's angry;\n",
            "Do strike at my queen: look down\n",
            "Do strike at my queen: look upon\n",
            "Ind see the hands, and there\n",
            "Will answerPer'd upon her\n",
            "My lord Paulina.\n",
            "\n",
            "PAULINA:\n",
            "I did not stumble.\n",
            "\n",
            "LEONTES:\n",
            "I told her so.\n",
            "\n",
            "PAULINA:\n",
            "It is; and is for she should be.\n",
            "\n",
            "PAULINA:\n",
            "Nor I, sir,\n",
            "That creep like to know't\n",
            "From your own accord I'll off;\n",
            "But first I'll do my errand. The good queen,\n",
            "For she is good, hath brought you forth a daughter;\n",
            "Here 'tis; commends it to your blessing.\n",
            "\n",
            "LEONTES:\n",
            "Out!\n",
            "A mankind witch! Hence with her, out o' door:\n",
            "A most intelligencing bawd!\n",
            "\n",
            "PAULINA:\n",
            "Not so:\n",
            "I am as ignorant in that as you\n",
            "In so entitling me, and no less honest\n",
            "Than you are mad; which is enough, I'll warrant,\n",
            "As this world goes, to pass for honest.\n",
            "\n",
            "LEONTES:\n",
            "Traitors!\n",
            "Will you not push her out? Give\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GLecDiHbogvX",
        "eo_QP1ITFfX2"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
